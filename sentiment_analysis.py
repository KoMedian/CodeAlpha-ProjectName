# -*- coding: utf-8 -*-
"""Sentiment Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i_sUTE5Ddu-Q_OPpmqdz3U4u-YtDP3iZ
"""

from google.colab import files # to import the files from the system
uploaded = files.upload()

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_csv("test.csv", encoding='latin-1')
df.info()

sns.countplot(x=df['sentiment'])

sns.countplot(x=df['sentiment'],hue=df['Age of User'])

df.isnull().sum()

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
def preprocess_text(text):

    if not isinstance(text, str):

        return ''

    text = re.sub(r'[^a-zA-Z\s]', '', text)

    text = text.lower()

    tokens = [word for word in text.split() if word not in stop_words]

    return ' '.join(tokens)

df['text'] = df['text'].fillna('') # Fill missing values with empty string
df['text']=df['text'].apply(preprocess_text)
df['text'] = df['text'].astype(str)

X=df['text']
y=df['sentiment']

import tensorflow ,keras
from tensorflow.keras.preprocessing.text import Tokenizer
token=Tokenizer(oov_token="'")
token.fit_on_texts(X)
(token.word_index)

max(token.word_index)

sentences=token.texts_to_sequences(X)
len(sentences)

from tensorflow.keras.preprocessing.sequence import pad_sequences
padded=pad_sequences(sentences,padding='pre')
padded.shape

from sklearn.preprocessing import LabelEncoder

# Drop rows with missing sentiment values
df.dropna(subset=['sentiment'], inplace=True)

encode=LabelEncoder()
y=encode.fit_transform(df['sentiment'])

from keras import Sequential
from keras.layers import Dense,LSTM , Dropout ,Embedding

model=Sequential()
model.add(Embedding(len(token.word_index) + 1, 2, input_length=padded.shape[1]))
model.add(LSTM(150))
model.add(Dense(16,activation='relu'))
model.add(Dense(3,activation='softmax'))

model.summary()

max(len(x) for x in sentences)

model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['Accuracy'])

model=Sequential()
model.add(Embedding(len(token.word_index) + 1, 2, input_length=padded.shape[1]))
model.add(LSTM(150))
model.add(Dense(16,activation='relu'))
model.add(Dense(3,activation='softmax'))

df.dropna(subset=['sentiment'], inplace=True)
X = df['text']
y = df['sentiment']

token = Tokenizer(oov_token="'")
token.fit_on_texts(X)
sentences = token.texts_to_sequences(X)
padded = pad_sequences(sentences, padding='pre', maxlen=21) # Use the determined maxlen

from sklearn.preprocessing import LabelEncoder

encode = LabelEncoder()
y = encode.fit_transform(y)

model.fit(padded, y, epochs=5)

# Read the test data from the uploaded file
test_df = pd.read_csv("test.csv", encoding='latin-1')

# Preprocess the text data in the test DataFrame
test_df['text'] = test_df['text'].fillna('')
test_df['text'] = test_df['text'].apply(preprocess_text)
test_sentences = token.texts_to_sequences(test_df['text'])
test_padded = pad_sequences(test_sentences, padding='pre', maxlen=padded.shape[1])
display(test_padded)

np.argmax(model.predict(test_padded))

df = pd.read_csv("test.csv", encoding='latin-1')
df.info()